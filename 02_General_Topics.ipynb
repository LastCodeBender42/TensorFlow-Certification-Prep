{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJegydEtsryg2xnjcWOoig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LastCodeBender42/TensorFlow-Certification-Prep/blob/main/02_General_Topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **01 Activation Functions in Neural Networks**\n",
        "\n",
        "An activation function in a neural network defines the output of a neuron given an input or set of inputs. It introduces non-linearity into the model, enabling it to learn and represent complex patterns in data. Without activation functions, a neural network would behave like a simple linear regression model, regardless of the number of layers.\n",
        "\n",
        "### Common Activation Functions\n",
        "\n",
        "1. **Sigmoid Function**:\n",
        "   \\[\n",
        "   \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "   \\]\n",
        "   - **Range**: (0, 1)\n",
        "   - **Usage**: Often used in binary classification problems.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "     x = tf.constant([-1.0, 0.0, 1.0])\n",
        "     sigmoid = tf.nn.sigmoid(x)\n",
        "     print(sigmoid.numpy())  # Output: [0.26894142, 0.5, 0.7310586]\n",
        "     ```\n",
        "\n",
        "2. **Tanh (Hyperbolic Tangent) Function**:\n",
        "   \\[\n",
        "   \\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1\n",
        "   \\]\n",
        "   - **Range**: (-1, 1)\n",
        "   - **Usage**: Commonly used in hidden layers of neural networks.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "     x = tf.constant([-1.0, 0.0, 1.0])\n",
        "     tanh = tf.nn.tanh(x)\n",
        "     print(tanh.numpy())  # Output: [-0.7615942, 0.0, 0.7615942]\n",
        "     ```\n",
        "\n",
        "3. **ReLU (Rectified Linear Unit) Function**:\n",
        "   \\[\n",
        "   \\text{ReLU}(x) = \\max(0, x)\n",
        "   \\]\n",
        "   - **Range**: [0, ∞)\n",
        "   - **Usage**: Widely used in hidden layers of neural networks due to its simplicity and effectiveness.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "     x = tf.constant([-1.0, 0.0, 1.0])\n",
        "     relu = tf.nn.relu(x)\n",
        "     print(relu.numpy())  # Output: [0.0, 0.0, 1.0]\n",
        "     ```\n",
        "\n",
        "4. **Leaky ReLU Function**:\n",
        "   \\[\n",
        "   \\text{Leaky ReLU}(x) =\n",
        "   \\begin{cases}\n",
        "   x & \\text{if } x > 0 \\\\\n",
        "   \\alpha x & \\text{if } x \\leq 0\n",
        "   \\end{cases}\n",
        "   \\]\n",
        "   - **Range**: (-∞, ∞)\n",
        "   - **Usage**: Used to solve the \"dying ReLU\" problem by allowing a small, non-zero gradient when the unit is not active.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "     x = tf.constant([-1.0, 0.0, 1.0])\n",
        "     leaky_relu = tf.nn.leaky_relu(x, alpha=0.1)\n",
        "     print(leaky_relu.numpy())  # Output: [-0.1, 0.0, 1.0]\n",
        "     ```\n",
        "\n",
        "### Importance of Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into the neural network, which allows it to learn and represent more complex patterns. Without non-linear activation functions, a neural network, regardless of its depth, would be equivalent to a single-layer perceptron, which can only model linear relationships.\n",
        "\n",
        "Each activation function has its own characteristics and is chosen based on the specific requirements of the model and the nature of the data. For instance, ReLU is popular for deep networks because it helps mitigate the vanishing gradient problem, while sigmoid and tanh are often used in the output layers of binary and multi-class classification problems, respectively.\n",
        "\n",
        "\n",
        "Let's consider a scenario where you are working with gene expression data to build a neural network model for classifying types of cancer based on the expression levels of various genes.\n",
        "\n",
        "### Example: Using Activation Functions with Gene Expression Data\n",
        "\n",
        "#### 1. Preparing the Data\n",
        "Suppose you have a dataset where each row represents a sample, each column represents a gene, and the target variable indicates the type of cancer. Here’s a simplified example:\n",
        "\n",
        "- **Features**: Gene expression levels (e.g., `gene1`, `gene2`, ..., `geneN`)\n",
        "- **Target**: Cancer type (e.g., 0 for type A, 1 for type B)\n",
        "\n",
        "#### 2. Building the Neural Network\n",
        "We'll build a neural network model using TensorFlow, where activation functions play a crucial role in transforming the input gene expression data through the network layers.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Example gene expression data (for illustration purposes)\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "num_samples = 100\n",
        "num_genes = 50\n",
        "X = np.random.rand(num_samples, num_genes)  # Gene expression levels\n",
        "y = np.random.randint(2, size=num_samples)  # Binary cancer type labels (0 or 1)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Building the neural network\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(num_genes,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Using sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
        "\n",
        "# Evaluating the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "### Explanation of Activation Functions Used\n",
        "\n",
        "1. **Input Layer**:\n",
        "   - The input layer receives the gene expression levels of each sample. These are numerical values representing the expression levels of each gene.\n",
        "\n",
        "2. **Hidden Layers**:\n",
        "   - **Dense Layer with ReLU Activation**: The first hidden layer has 64 units with ReLU activation. ReLU (`tf.nn.relu`) transforms the input values by setting all negative values to 0, introducing non-linearity to help the model learn complex patterns.\n",
        "   - **Dense Layer with ReLU Activation**: The second hidden layer has 32 units with ReLU activation, further transforming the data.\n",
        "\n",
        "3. **Output Layer**:\n",
        "   - **Dense Layer with Sigmoid Activation**: The output layer has 1 unit with sigmoid activation (`tf.nn.sigmoid`). The sigmoid function maps the output to a range between 0 and 1, which is suitable for binary classification tasks. It outputs the probability of the sample belonging to class 1 (e.g., cancer type B).\n",
        "\n",
        "### Importance in Gene Expression Data\n",
        "\n",
        "- **Non-linearity**: Activation functions like ReLU introduce non-linearity, which allows the neural network to model complex relationships between gene expression levels and cancer types.\n",
        "- **Probability Output**: The sigmoid activation in the output layer is crucial for binary classification tasks, providing a probability score that can be interpreted as the likelihood of a sample belonging to a specific cancer type.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Activation functions play a vital role in transforming gene expression data through the layers of a neural network. ReLU is typically used in hidden layers to handle non-linearities, while sigmoid (or softmax for multi-class classification) is used in the output layer to produce probability scores for classification tasks. This example demonstrates how you can build and train a neural network to classify cancer types based on gene expression data, leveraging activation functions to enhance the model's learning capabilities."
      ],
      "metadata": {
        "id": "SP4r9eL689B_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **02 Tensor Gradients**\n",
        "\n",
        "The gradient output `[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=8.0>]` indicates the gradients of a loss function with respect to two variables (or tensors). Each element in the list is a tensor representing the gradient of the loss with respect to one of the variables.\n",
        "\n",
        "### Understanding the Gradient Output\n",
        "\n",
        "1. **Shape**: `shape=()` indicates that the gradient is a scalar (0-dimensional tensor). This is common when the variables being differentiated are scalars themselves.\n",
        "2. **dtype**: `dtype=float32` indicates the data type of the gradient values, which in this case is a 32-bit floating-point number.\n",
        "3. **numpy**: This provides the actual numeric value of the gradient. In this case, `numpy=6.0` and `numpy=8.0`.\n",
        "\n",
        "### Context\n",
        "\n",
        "To understand this better, let's look at an example where such a gradient output might be generated.\n",
        "\n",
        "### Example: Computing Gradients\n",
        "\n",
        "Let's say we have a simple function \\( f(x, y) = 3x^2 + 4y \\), and we want to compute the gradients with respect to \\( x \\) and \\( y \\).\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the variables\n",
        "x = tf.Variable(1.0)\n",
        "y = tf.Variable(2.0)\n",
        "\n",
        "# Define the function\n",
        "with tf.GradientTape() as tape:\n",
        "    f = 3 * x**2 + 4 * y\n",
        "\n",
        "# Compute the gradients\n",
        "gradients = tape.gradient(f, [x, y])\n",
        "\n",
        "# Print the gradients\n",
        "print(gradients)\n",
        "```\n",
        "\n",
        "### Output\n",
        "```\n",
        "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "- The function \\( f(x, y) = 3x^2 + 4y \\) is defined.\n",
        "- Using `tf.GradientTape`, TensorFlow tracks the computation to compute the gradients.\n",
        "- `tape.gradient(f, [x, y])` computes the gradients of \\( f \\) with respect to `x` and `y`.\n",
        "\n",
        "The gradients are calculated as follows:\n",
        "- The partial derivative of \\( f \\) with respect to \\( x \\): \\( \\frac{\\partial f}{\\partial x} = 6x \\). For \\( x = 1 \\), this is \\( 6 \\times 1 = 6.0 \\).\n",
        "- The partial derivative of \\( f \\) with respect to \\( y \\): \\( \\frac{\\partial f}{\\partial y} = 4 \\). This is a constant 4, so regardless of \\( y \\), it is \\( 4.0 \\).\n",
        "\n",
        "Therefore, the gradient output `[6.0, 4.0]` matches the computed gradients:\n",
        "- `6.0` is the gradient of \\( f \\) with respect to \\( x \\).\n",
        "- `4.0` is the gradient of \\( f \\) with respect to \\( y \\).\n",
        "\n",
        "### Summary\n",
        "The gradient output `[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=8.0>]` means that:\n",
        "- The gradient of the loss (or function being differentiated) with respect to the first variable is `6.0`.\n",
        "- The gradient of the loss with respect to the second variable is `8.0`.\n",
        "These gradients indicate how much the loss will change with a small change in each variable, guiding the optimization process in machine learning models."
      ],
      "metadata": {
        "id": "XqMgSNpz9Ptk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **03 One-hot Encoding**\n",
        "\n",
        "One-hot encoding is a technique used to convert categorical data into a numerical format that can be used by machine learning algorithms. It is particularly useful when you have categorical variables with no intrinsic order (i.e., nominal data). In one-hot encoding, each category is represented by a binary vector where only one element is `1` (hot) and all other elements are `0`.\n",
        "\n",
        "### Example\n",
        "\n",
        "Suppose you have a categorical variable with three categories: \"red,\" \"green,\" and \"blue.\" Here’s how one-hot encoding would transform this data:\n",
        "\n",
        "1. **Categories**: [\"red\", \"green\", \"blue\"]\n",
        "2. **One-Hot Encoded Representation**:\n",
        "   - \"red\" -> `[1, 0, 0]`\n",
        "   - \"green\" -> `[0, 1, 0]`\n",
        "   - \"blue\" -> `[0, 0, 1]`\n",
        "\n",
        "### Why Use One-Hot Encoding?\n",
        "\n",
        "- **Machine Learning Algorithms**: Many machine learning algorithms cannot work with categorical data directly and require numerical input.\n",
        "- **No Ordinal Relationship**: One-hot encoding is appropriate when there is no ordinal relationship between categories (e.g., colors, countries).\n",
        "\n",
        "### How to Perform One-Hot Encoding in TensorFlow\n",
        "\n",
        "Let’s go through an example using TensorFlow to perform one-hot encoding.\n",
        "\n",
        "#### Example in TensorFlow\n",
        "\n",
        "Suppose you have the following categories: [\"cat\", \"dog\", \"bird\"].\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the categories\n",
        "categories = [\"cat\", \"dog\", \"bird\"]\n",
        "\n",
        "# Convert categories to indices (e.g., using a mapping)\n",
        "category_to_index = {category: index for index, category in enumerate(categories)}\n",
        "\n",
        "# List of categories to encode\n",
        "category_list = [\"cat\", \"dog\", \"bird\", \"cat\"]\n",
        "\n",
        "# Convert category list to indices\n",
        "indices = [category_to_index[category] for category in category_list]\n",
        "\n",
        "# Perform one-hot encoding\n",
        "one_hot_encoded = tf.one_hot(indices, depth=len(categories))\n",
        "\n",
        "# Print the result\n",
        "print(one_hot_encoded)\n",
        "```\n",
        "\n",
        "### Output\n",
        "```\n",
        "tf.Tensor(\n",
        "[[1. 0. 0.]\n",
        " [0. 1. 0.]\n",
        " [0. 0. 1.]\n",
        " [1. 0. 0.]], shape=(4, 3), dtype=float32)\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Mapping Categories to Indices**:\n",
        "   ```python\n",
        "   category_to_index = {category: index for index, category in enumerate(categories)}\n",
        "   ```\n",
        "   This creates a dictionary that maps each category to a unique index: `{\"cat\": 0, \"dog\": 1, \"bird\": 2}`.\n",
        "\n",
        "2. **Convert Category List to Indices**:\n",
        "   ```python\n",
        "   indices = [category_to_index[category] for category in category_list]\n",
        "   ```\n",
        "   This converts the list of categories to their corresponding indices: `[0, 1, 2, 0]`.\n",
        "\n",
        "3. **One-Hot Encoding**:\n",
        "   ```python\n",
        "   one_hot_encoded = tf.one_hot(indices, depth=len(categories))\n",
        "   ```\n",
        "   This converts the indices to one-hot encoded vectors. The `depth` parameter specifies the number of categories.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **One-Hot Encoding**: Converts categorical data into a binary vector where only one element is `1` and the rest are `0`.\n",
        "- **Usage**: Useful for machine learning algorithms that require numerical input and when there is no ordinal relationship between categories.\n",
        "- **TensorFlow**: The `tf.one_hot` function can be used to perform one-hot encoding efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "T4n4Gh9I9xiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **04 Tensor Axes**\n",
        "\n",
        "In TensorFlow, an axis is a dimension along which operations like reductions (sum, mean, etc.) and manipulations (slicing, reshaping) are performed. Understanding axes is crucial for working with tensors, especially when performing operations that aggregate or reshape data.\n",
        "\n",
        "### Description of Axis\n",
        "\n",
        "- **Axis**: An axis in a tensor refers to a specific dimension along which operations can be performed. For instance, in a 2D tensor (matrix), axis `0` typically refers to the rows and axis `1` refers to the columns. In a 3D tensor, axis `0` could be the depth, axis `1` could be the rows, and axis `2` could be the columns.\n",
        "\n",
        "- **Axes in Tensor Operations**: When performing operations such as summing, averaging, or applying functions, you often need to specify the axis along which the operation should be performed. For example, summing along axis `0` will reduce the tensor along the rows, collapsing them into a single value per column.\n",
        "\n",
        "### Examples\n",
        "\n",
        "#### 1. **Reducing a Tensor along an Axis**\n",
        "\n",
        "Consider a 2D tensor (matrix) and let's perform a sum operation along a specific axis.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a 2D tensor (matrix) with shape (3, 4)\n",
        "tensor = tf.constant([[1, 2, 3, 4],\n",
        "                      [5, 6, 7, 8],\n",
        "                      [9, 10, 11, 12]])\n",
        "\n",
        "# Sum along axis 0 (columns)\n",
        "sum_axis_0 = tf.reduce_sum(tensor, axis=0)\n",
        "print(\"Sum along axis 0:\", sum_axis_0)\n",
        "# Output: [15, 18, 21, 24]\n",
        "\n",
        "# Sum along axis 1 (rows)\n",
        "sum_axis_1 = tf.reduce_sum(tensor, axis=1)\n",
        "print(\"Sum along axis 1:\", sum_axis_1)\n",
        "# Output: [10, 26, 42]\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- **Axis 0**: Summing along axis `0` collapses rows, resulting in the sum of each column.\n",
        "- **Axis 1**: Summing along axis `1` collapses columns, resulting in the sum of each row.\n",
        "\n",
        "#### 2. **Slicing a Tensor along an Axis**\n",
        "\n",
        "You can also slice a tensor along a particular axis.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a 3D tensor with shape (2, 3, 4)\n",
        "tensor = tf.constant([[[1, 2, 3, 4],\n",
        "                       [5, 6, 7, 8],\n",
        "                       [9, 10, 11, 12]],\n",
        "\n",
        "                      [[13, 14, 15, 16],\n",
        "                       [17, 18, 19, 20],\n",
        "                       [21, 22, 23, 24]]])\n",
        "\n",
        "# Slice along axis 0\n",
        "slice_axis_0 = tensor[1, :, :]\n",
        "print(\"Slice along axis 0:\", slice_axis_0)\n",
        "# Output: [[13, 14, 15, 16],\n",
        "#          [17, 18, 19, 20],\n",
        "#          [21, 22, 23, 24]]\n",
        "\n",
        "# Slice along axis 1\n",
        "slice_axis_1 = tensor[:, 1, :]\n",
        "print(\"Slice along axis 1:\", slice_axis_1)\n",
        "# Output: [[ 5,  6,  7,  8],\n",
        "#          [17, 18, 19, 20]]\n",
        "\n",
        "# Slice along axis 2\n",
        "slice_axis_2 = tensor[:, :, 2]\n",
        "print(\"Slice along axis 2:\", slice_axis_2)\n",
        "# Output: [[ 3,  7, 11],\n",
        "#          [15, 19, 23]]\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- **Axis 0**: Selecting a slice along axis `0` extracts one \"layer\" from the tensor.\n",
        "- **Axis 1**: Selecting a slice along axis `1` extracts a slice of all layers but with the specified row.\n",
        "- **Axis 2**: Selecting a slice along axis `2` extracts a slice of all layers and rows but with the specified column.\n",
        "\n",
        "### Visualizing Axes\n",
        "\n",
        "In general:\n",
        "- For a tensor with shape `(D1, D2, ..., Dn)`, axis `0` is the first dimension, axis `1` is the second dimension, and so on up to axis `n-1` for the last dimension.\n",
        "\n",
        "Understanding axes helps in effective tensor manipulation and operations, making it easier to perform complex data transformations and aggregations. If you have more questions or need further examples, feel free to ask!\n",
        "\n",
        "Yes, exactly! The tensor shape `(2, 3, 4)` can indeed be interpreted in the context of gene expression data. Here’s how it maps to your example:\n",
        "\n",
        "### Interpretation of Tensor Shape `(2, 3, 4)`\n",
        "\n",
        "1. **Dimension 0 (Size 2)**: Represents the number of different gene expression profiles or conditions. In your example, this could be two different gene expression profiles.\n",
        "\n",
        "2. **Dimension 1 (Size 3)**: Represents the number of replicates for each gene expression profile. So, you have three replicates for each profile.\n",
        "\n",
        "3. **Dimension 2 (Size 4)**: Represents the time points at which the gene expression was measured. You have four time points for each replicate.\n",
        "\n",
        "### Example Breakdown\n",
        "\n",
        "- **Profile 1**:\n",
        "  - **Replicate 1**: Measurements at time points 1, 2, 3, 4\n",
        "  - **Replicate 2**: Measurements at time points 1, 2, 3, 4\n",
        "  - **Replicate 3**: Measurements at time points 1, 2, 3, 4\n",
        "\n",
        "- **Profile 2**:\n",
        "  - **Replicate 1**: Measurements at time points 1, 2, 3, 4\n",
        "  - **Replicate 2**: Measurements at time points 1, 2, 3, 4\n",
        "  - **Replicate 3**: Measurements at time points 1, 2, 3, 4\n",
        "\n",
        "Here’s a visual representation:\n",
        "\n",
        "```\n",
        "Gene Expression Profile 1:\n",
        "[\n",
        " [ [Time1_Replicate1, Time2_Replicate1, Time3_Replicate1, Time4_Replicate1],\n",
        "   [Time1_Replicate2, Time2_Replicate2, Time3_Replicate2, Time4_Replicate2],\n",
        "   [Time1_Replicate3, Time2_Replicate3, Time3_Replicate3, Time4_Replicate3] ]\n",
        "]\n",
        "\n",
        "Gene Expression Profile 2:\n",
        "[\n",
        " [ [Time1_Replicate1, Time2_Replicate1, Time3_Replicate1, Time4_Replicate1],\n",
        "   [Time1_Replicate2, Time2_Replicate2, Time3_Replicate2, Time4_Replicate2],\n",
        "   [Time1_Replicate3, Time2_Replicate3, Time3_Replicate3, Time4_Replicate3] ]\n",
        "]\n",
        "```\n",
        "\n",
        "### Operations on This Tensor\n",
        "\n",
        "- **Sum/Mean Across Time Points (Axis 2)**: To aggregate expression values across time points, you might perform operations along axis `2`, resulting in a tensor of shape `(2, 3)` where each element represents the sum or mean expression across the four time points.\n",
        "\n",
        "- **Average Across Replicates (Axis 1)**: To get the average gene expression profile across replicates for each profile, you would perform operations along axis `1`, resulting in a tensor of shape `(2, 4)` where each element represents the average expression across replicates.\n",
        "\n",
        "- **Profile Comparisons (Axis 0)**: To compare gene expression profiles, you might perform operations across axis `0`, aggregating or comparing across the two profiles.\n",
        "\n",
        "This dimensionality allows you to structure and analyze gene expression data effectively, considering both the biological conditions and the experimental setup."
      ],
      "metadata": {
        "id": "oByD8irL-JTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **05 Reshaping Tensors**\n",
        "\n",
        "Using `-1` in the reshape function is a convenient way to specify that TensorFlow should automatically determine the size of that particular dimension based on the total number of elements and the sizes of other dimensions. Here are some reasons why using `-1` is useful:\n",
        "\n",
        "1. **Flexibility and Convenience:**\n",
        "   - You don't need to manually calculate the size of the dimension you want TensorFlow to infer.\n",
        "   - This is particularly helpful when working with tensors whose shapes might change dynamically during the program execution.\n",
        "\n",
        "2. **Avoiding Errors:**\n",
        "   - It reduces the risk of making mistakes in manually computing the new shape, especially in complex transformations or when dealing with higher-dimensional tensors.\n",
        "   - TensorFlow ensures that the total number of elements remains the same, thus preventing reshape errors.\n",
        "\n",
        "3. **Code Readability:**\n",
        "   - Using `-1` can make the code more readable and maintainable, as it clearly indicates that TensorFlow should determine the appropriate size for that dimension.\n",
        "\n",
        "### Example Scenarios:\n",
        "\n",
        "#### 1. Flattening a Tensor:\n",
        "```python\n",
        "tensor = tf.reshape(tf.range(1, 17), (4, 4))\n",
        "flattened_tensor = tf.reshape(tensor, [-1])\n",
        "```\n",
        "Here, `[-1]` specifies that the tensor should be reshaped into a one-dimensional tensor with all elements from the original tensor.\n",
        "\n",
        "#### 2. Dynamic Shape Transformations:\n",
        "Suppose you have a batch of images where the batch size can vary:\n",
        "```python\n",
        "batch_size = tf.placeholder(tf.int32, shape=[])\n",
        "images = tf.placeholder(tf.float32, shape=[None, 28, 28, 3])  # Batch of 28x28 RGB images\n",
        "flattened_images = tf.reshape(images, [batch_size, -1])\n",
        "```\n",
        "In this example, `-1` allows TensorFlow to automatically compute the number of features per image (which is `28 * 28 * 3 = 2352`), regardless of the batch size.\n",
        "\n",
        "#### 3. Reshaping for Model Layers:\n",
        "```python\n",
        "input_tensor = tf.placeholder(tf.float32, shape=[None, 784])  # Batch of flattened 28x28 grayscale images\n",
        "reshaped_tensor = tf.reshape(input_tensor, [-1, 28, 28, 1])\n",
        "```\n",
        "Here, `-1` allows TensorFlow to adjust the batch size dynamically while reshaping the flattened images back to their original 28x28 shape with 1 channel.\n",
        "\n",
        "### Summary:\n",
        "Using `-1` simplifies tensor reshaping by letting TensorFlow handle the size calculation for one dimension, ensuring the total number of elements is preserved. This leads to more flexible, error-resistant, and readable code."
      ],
      "metadata": {
        "id": "bohg7z6e-pNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **06 Tensor Permutation and Transposition**\n",
        "\n",
        "you can think of `tf.transpose` with the `perm` argument as similar to reshaping in that it changes the way data is organized. However, they are fundamentally different operations:\n",
        "\n",
        "- **Reshaping**: Changes the shape of the tensor by rearranging the data in a new structure without changing the total number of elements. It can collapse or expand dimensions but preserves the data order.\n",
        "\n",
        "- **Transposing with Permutation**: Changes the order of dimensions (axes) without changing the data within each dimension. It effectively reorders the axes based on the permutation specified.\n",
        "\n",
        "### Example to Illustrate the Difference\n",
        "\n",
        "#### Original Tensor\n",
        "Consider a tensor of shape `(2, 3, 4)`:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "tensor = tf.constant([\n",
        "    [[ 1,  2,  3,  4],\n",
        "     [ 5,  6,  7,  8],\n",
        "     [ 9, 10, 11, 12]],\n",
        "    \n",
        "    [[13, 14, 15, 16],\n",
        "     [17, 18, 19, 20],\n",
        "     [21, 22, 23, 24]]\n",
        "])\n",
        "\n",
        "print(\"Original shape:\", tensor.shape)\n",
        "print(tensor)\n",
        "```\n",
        "\n",
        "### Reshaping\n",
        "Reshaping changes the shape but keeps the element order:\n",
        "```python\n",
        "reshaped_tensor = tf.reshape(tensor, (4, 6))\n",
        "print(\"Reshaped shape:\", reshaped_tensor.shape)\n",
        "print(reshaped_tensor)\n",
        "```\n",
        "Output:\n",
        "```\n",
        "Original shape: (2, 3, 4)\n",
        "tf.Tensor(\n",
        "[[[ 1  2  3  4]\n",
        "  [ 5  6  7  8]\n",
        "  [ 9 10 11 12]]\n",
        "\n",
        " [[13 14 15 16]\n",
        "  [17 18 19 20]\n",
        "  [21 22 23 24]]], shape=(2, 3, 4), dtype=int32)\n",
        "Reshaped shape: (4, 6)\n",
        "tf.Tensor(\n",
        "[[ 1  2  3  4  5  6]\n",
        " [ 7  8  9 10 11 12]\n",
        " [13 14 15 16 17 18]\n",
        " [19 20 21 22 23 24]], shape=(4, 6), dtype=int32)\n",
        "```\n",
        "\n",
        "### Transposing with Permutation\n",
        "Transposing with permutation changes the axes order:\n",
        "```python\n",
        "transposed_tensor = tf.transpose(tensor, perm=[1, 2, 0])\n",
        "print(\"Transposed shape:\", transposed_tensor.shape)\n",
        "print(transposed_tensor)\n",
        "```\n",
        "Output:\n",
        "```\n",
        "Transposed shape: (3, 4, 2)\n",
        "tf.Tensor(\n",
        "[[[ 1 13]\n",
        "  [ 2 14]\n",
        "  [ 3 15]\n",
        "  [ 4 16]]\n",
        "\n",
        " [[ 5 17]\n",
        "  [ 6 18]\n",
        "  [ 7 19]\n",
        "  [ 8 20]]\n",
        "\n",
        " [[ 9 21]\n",
        "  [10 22]\n",
        "  [11 23]\n",
        "  [12 24]]], shape=(3, 4, 2), dtype=int32)\n",
        "```\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "- **Reshaping**:\n",
        "  - Changes the shape while preserving the element order.\n",
        "  - The total number of elements remains the same.\n",
        "  - Useful for preparing data for different layers in neural networks.\n",
        "\n",
        "- **Transposing with Permutation**:\n",
        "  - Changes the order of dimensions (axes).\n",
        "  - Does not change the number of elements or their internal order within each axis.\n",
        "  - Useful for changing data layout, often required for operations like matrix multiplication or preparing data for specific layer types in neural networks.\n",
        "\n",
        "### Summary\n",
        "While both reshaping and transposing with permutation change the structure of the tensor, reshaping alters the arrangement of the elements to fit a new shape, whereas transposing with permutation reorders the axes of the tensor. Each operation is useful in different contexts depending on the desired data layout and operations to be performed."
      ],
      "metadata": {
        "id": "05drxkGR-58U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyJFU1Z88_MH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}