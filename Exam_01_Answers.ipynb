{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8mK40U+Nl+5LhlqJl+D/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LastCodeBender42/TensorFlow-Certification-Prep/blob/main/Exam_01_Answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Certification Mock Exam 01\n",
        "\n",
        "### Question 1: Basic Tensor Operations\n",
        "**Exercise 1.1**: Create a tensor of shape `(4, 4)` with random values. Compute the element-wise square of this tensor and then compute the mean of all the elements.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a tensor of shape (4, 4) with random values\n",
        "tensor = tf.random.uniform((4, 4))\n",
        "\n",
        "# Compute the element-wise square\n",
        "squared_tensor = tf.square(tensor)\n",
        "\n",
        "# Compute the mean of all elements\n",
        "mean_value = tf.reduce_mean(squared_tensor)\n",
        "\n",
        "print(\"Mean of squared tensor:\", mean_value.numpy())\n",
        "```\n",
        "\n",
        "### Question 2: Data Pipeline\n",
        "**Exercise 1.2**: Create a `tf.data.Dataset` from a list of numbers from 0 to 9. Batch the dataset into batches of size 3 and then shuffle the dataset with a buffer size of 5.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a dataset from a list of numbers\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "\n",
        "# Batch the dataset\n",
        "batched_dataset = dataset.batch(3)\n",
        "\n",
        "# Shuffle the dataset\n",
        "shuffled_dataset = batched_dataset.shuffle(buffer_size=5)\n",
        "\n",
        "for batch in shuffled_dataset:\n",
        "    print(batch.numpy())\n",
        "```\n",
        "\n",
        "### Question 3: Model Building\n",
        "**Exercise 1.3**: Build a sequential model with two hidden layers (first with 64 units and ReLU activation, second with 32 units and ReLU activation) and an output layer with 10 units (softmax activation). Compile the model with sparse categorical crossentropy loss and the Adam optimizer.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(100,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "### Question 4: Image Classification\n",
        "**Exercise 1.4**: Load the MNIST dataset from TensorFlow datasets, normalize the pixel values to be between 0 and 1, and train the model built in Exercise 1.53 for 5 epochs.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load and prepare the dataset\n",
        "(ds_train, ds_test), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "\n",
        "# Normalize the pixel values\n",
        "def normalize_img(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255.0, label\n",
        "\n",
        "ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(32)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.batch(32)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Train the model\n",
        "model.fit(ds_train, epochs=5, validation_data=ds_test)\n",
        "```\n",
        "\n",
        "### Question 5: Natural Language Processing\n",
        "**Exercise 1.5**: Create a tokenizer using TensorFlow's `Tokenizer` and fit it on a list of sample sentences. Convert a new sentence to its corresponding sequence of integers.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    'I love machine learning',\n",
        "    'Deep learning is a subset of machine learning',\n",
        "    'Natural language processing is interesting'\n",
        "]\n",
        "\n",
        "# Create and fit the tokenizer\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# New sentence to convert\n",
        "new_sentence = 'I love deep learning'\n",
        "\n",
        "# Convert to sequence of integers\n",
        "sequence = tokenizer.texts_to_sequences([new_sentence])\n",
        "\n",
        "print(\"Sequence of integers:\", sequence)\n",
        "```\n",
        "\n",
        "### Question 6: Time Series Prediction\n",
        "**Exercise 1.6**: Create a simple LSTM model to predict the next value in a sequence of numbers. Use a single LSTM layer with 50 units followed by a dense layer.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Create the model\n",
        "model = models.Sequential([\n",
        "    layers.LSTM(50, activation='relu', input_shape=(10, 1)),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "### Question 7: Model Saving and Loading\n",
        "**Exercise 1.7**: Train a model and save the weights to a file. Load the weights back into the model and evaluate it on the test data.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Create the model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(100,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on dummy data\n",
        "import numpy as np\n",
        "x_train = np.random.random((1000, 100))\n",
        "y_train = np.random.randint(10, size=(1000,))\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Save the weights\n",
        "model.save_weights('model_weights.h5')\n",
        "\n",
        "# Load the weights into a new model\n",
        "new_model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(100,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "new_model.load_weights('model_weights.h5')\n",
        "\n",
        "# Evaluate the new model on the same dummy data\n",
        "loss, acc = new_model.evaluate(x_train, y_train)\n",
        "print(\"Restored model accuracy:\", acc)\n",
        "```\n",
        "\n",
        "### Question 8: Transfer Learning\n",
        "**Exercise 1.8**: Use a pre-trained MobileNetV2 model for transfer learning. Freeze the base model and add a new dense layer with 10 units for classification. Compile and train the model on a small dataset.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add new layers on top of the base model\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create dummy data for training\n",
        "x_train = np.random.random((100, 224, 224, 3))\n",
        "y_train = np.random.randint(10, size=(100,))\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "### Question 9: Custom Training Loop\n",
        "**Exercise 1.9**: Implement a custom training loop for a simple neural network. Use the `GradientTape` to compute gradients and update the model weights manually.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Create the model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(100,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training data\n",
        "x_train = np.random.random((1000, 100))\n",
        "y_train = np.random.randint(10, size=(1000,))\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch = x_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    print(f'Epoch {epoch+1} completed')\n",
        "\n",
        "# Evaluate the model\n",
        "loss, acc = model.evaluate(x_train, y_train)\n",
        "print(\"Model accuracy after custom training loop:\", acc)\n",
        "```\n",
        "\n",
        "### Question 10: Model Deployment\n",
        "**Exercise 1.12**: Export a trained model to TensorFlow SavedModel format and then load it back to make a prediction.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Create the model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(100,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense\n",
        "\n",
        "(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on dummy data\n",
        "x_train = np.random.random((1000, 100))\n",
        "y_train = np.random.randint(10, size=(1000,))\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Save the model\n",
        "model.save('saved_model')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('saved_model')\n",
        "\n",
        "# Make a prediction with the loaded model\n",
        "predictions = loaded_model.predict(x_train[:5])\n",
        "print(\"Predictions on the first 5 samples:\", predictions)\n",
        "```\n"
      ],
      "metadata": {
        "id": "qaEvaXTrxrSb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvCIAYvUx2QH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}